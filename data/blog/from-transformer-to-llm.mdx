---
title: From Transformer to LLMs
date: '2024-12-12'
tags: ['LLMs','Language Models','Transformer', 'Fine-tuning', 'RAG']
draft: true
summary: 'The transformer is a neural network with a specific structure that includes a mechanism called self-attention or more specifically, multi-head attention.'
image: '/static/images/resource/from-transformer-to-llm/thumbnail-board-room-attention.png'
---

<p className="note">
  *The thumbnail is sourced from [A Deep Dive into Transformers with TensorFlow and Keras: Part 1](https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/).*
</p>

<TOCInline toc={props.toc} asDisclosure toHeading={3} />

The transformer is a neural network with a specific structure that includes a mechanism called **self-attention** or more specifically, **multi-head attention**.  This allows the model to build rich contextual representations of a token’s meaning by attending to and integrating information from surrounding tokens, effectively capturing how tokens relate to each other over long spans.

# I. The Transformer

## 1. Attention

### 1.1. Self-attention

Attention is a mechanism used in transformers to help a model focus on relevant parts of the input sequence when creating representations for each token. Instead of processing each token independently, attention allows the model to weigh and combine information from other tokens in the sequence. This process helps the model capture relationships between tokens and build better context-aware representations.

In a transformer, attention works by taking the representation of a token at layer $k-1$ and computing how much each of the other tokens should contribute to the token’s new representation at layer $k$. This is done by calculating attention scores, which reflect the relevance of each token to the one being processed.

Self-attention is a type of attention mechanism where each token in the sequence attends to (or "pays attention to") every other token, including itself. This allows the model to capture dependencies between tokens, regardless of their position in the sequence.

In self-attention, each token in the sequence is represented by three different types of vectors that capture distinct roles during the attention process. These roles are query, key, and value, and each token plays one of these roles at any given time.

To compute self-attention, transformers introduce three sets of weight matrices: $W^Q$, $W^K$, and $W^V$. These weight matrices are used to transform the input token representation $g_i$ into the query, key, and value vectors.

- Query vector $g_t$ is used to compute the attention scores with other tokens to determine which ones are most relevant to the current token.
  $$
  q_i=W^Q \cdot g_i
  $$
- The key vector $g_c$ helps to measure the similarity between tokens, allowing the model to decide how much attention to give each token based on the query-key comparison.
  $$
  k_i=W^K \cdot g_i
  $$
- The value vector $g_i$ contains the information (or "value") that will be aggregated and passed to the next layer
  $$
  v_i= V \cdot g_i
  $$

The self-attention mechanism computes attention scores by comparing the query vector of the current token with the key vectors of the previous tokens, then using these scores to weight the value vectors of those tokens. The weighted sum of the value vectors becomes the output for the current token.

To calculate the attention scores $\alpha$, which determine how much focus one token should have on another, we first compute the dot product between the query vector $q_t$ of the current token $t$ and the key vector $k_c$ of the other token $c$:

$$
\alpha_{t,c} = \text{softmax}\left(q_t \cdot k_c\right)
$$

However, the raw dot product can result in large values, especially as the model dimensionality increases, which could lead to gradient instability during training. To mitigate this, we scale the dot product by the square root of the dimensionality of the key vectors, $d_k$, which helps maintain stable gradients:

$$
\alpha_{t,c} = \text{softmax}\left(\frac{q_t \cdot k_c}{\sqrt{d_k}}\right)
$$

After computing the attention scores, we use them to weight the value vectors $v_i$ of all tokens. The final output for a token $t$, denoted as $h_t$, is the weighted sum of the value vectors, where the weights are the attention scores:

$$
h_t = \sigma \left( \sum_{i=1}^n \text{softmax}\left( \frac{\mathbf{q}_t \cdot \mathbf{k}_i}{\sqrt{d_k}} \right) \cdot \mathbf{v}_i \right)
$$

This output, $h_t$, represents the context-aware embedding of token $t$, combining information from other tokens in the sequence, weighted by how relevant each token is to $t$.

### 1.2. Multi-head attention

Multi-head attention is a mechanism within transformers that extends self-attention by allowing the model to attend to different aspects of the input simultaneously. This is achieved by using multiple attention heads, each operating independently but in parallel.
- One for syntactic relatedness
- One for semantic relatedness
- One for coreference, etc

<p align="center">
  <img src="/static/images/resource/from-transformer-to-llm/transformer_self-attention_viz_1.png" alt="multi-head-attention" />
</p>

<p style={{ textAlign: "center" }}>*Source: http://jalammar.github.io/illustrated-transformer*</p>

In multi-head attention, the attention mechanism is applied multiple times in parallel, with each attention "head" using its own set of learned parameters. Each head independently computes attention using its own query, key, and value matrices:

- Query vector: $q_i^h=W^{Q,h} \cdot g_i$
- Key vector: $k_i^h=W^{K,h}\cdot g_i$
- Value vector: $v_i^h= V^h \cdot g_i$
  
For each head $h$, the attention scores are computed as:

$$
\text{head}_t^h = \sum_{i=1}^n \text{softmax} \left( \frac{\mathbf{q}_t^h \cdot \mathbf{k}_i^h}{\sqrt{d_k}} \right) \cdot \mathbf{v}_i^h
$$

Concatenate all head vectors and project then

$$
h_t = \left( \text{head}_t^1 \oplus \cdots \oplus \text{head}_t^m \right) \cdot W^O
$$

## 2. Position embeddings

TBD

Self-attention does not have any notion of word ordering

→ **Simple solution:** Concatenate semantic word embedding with absolute position embedding

![position_embedding](/static/images/resource/from-transformer-to-llm/position_embedding_viz_2.png)

Multi-head self-attention isn’t quite sufficient. We need a few extra things and package everything up into so-called Transformer blocks

## 3. Transformer blocks

TBD

## 4. Types of Transformer models

### 4.1. Sequence Encoders with self-attention

Encoder self-attention covers the entire sequence, i.e. both the words to the left of the current position and the words to the right of the current position.

- Text classification (one label per sentence)
  - Train a model on the MLM task, adding a `[CLS]` token in front of every sentence
  - Throw away the output layer, create a new one
  - Fine-tune the model to predict the label at the `[CLS]` position

![encoder_text_classification](/static/images/resource/from-transformer-to-llm/encoder_text_classification_viz_3.png)

- Sequence labeling
  - Train a model on the MLM task
  - Throw away the output layer, create a new one
  - Fine-tune the model on POS-annotated data

![encoder_sequence_labeling](/static/images/resource/from-transformer-to-llm/encoder_sequence_labeling_viz_4.png)

### 4.2. Sequence Decoders with self-attention

- Attention can only access tokens to the left.
- But typically, far-way tokens get less attention.
- Decoder self-attention only covers the words to the left of the current position (because the words to the right are not yet known at test time).

![decoder](/static/images/resource/from-transformer-to-llm/decoder_viz_5.png)

### 4.3. Encoder-Decoder model with cross-attention

Main idea in Machine Translation

- Train an encoder for the source language
- Train a decoder for the target language
- Connecting
    - First Idea: it can be work if we have the same word order of source and target language
        
        ![mt_first_idea](/static/images/resource/from-transformer-to-llm/mt_first_idea_viz_6.png)
     
    - So that’s why we need **cross-attention**
        
        ![mt_cross_attention](/static/images/resource/from-transformer-to-llm/mt_cross_attention_viz_7.png)
        

![full_transformer](/static/images/resource/from-transformer-to-llm/full_transformer_viz_8.png)


# II. Large Language Models (LLMs)

## 1. Definition

> LLMs = **Machine learning models** optimized to **predict the next word** in text
> - **Machine learning models** -> Deep Neural net (Transformer architecture) with many billions of parameters
> - **predict the next word** -> trained on huge collections of texts crawled from the web

- Key mechanism: **self-attention**
- Input: sequence of tokens
- Output: probabilities for the next token

## 2. Training Process

- The LLM is trained through **self-supervision**:

  1. Given a text made of tokens $[w_1, w_2, \dots, w_n]$, we take partial sequences $[w_1, w_2, \dots, w_i]$ and seek to predict the next token $w_{i+1}$.
  2. The LLM will take as input $[w_1, w_2, \dots, w_i]$ and output a **probability distribution** $\hat{y}_i$ over possible next tokens.
  3. The loss for this token is then defined as the cross-entropy:
     $$
     L_{CE} = -\log \hat{y}_i[w_{i+1}]
     $$

     - **Explanation:** Probability of the actual token $w_{i+1}$ in the output distribution $\hat{y}_i$.
  4. We then make (at batch level) a small update on the model weights $\theta$ to reduce the loss.

- **Basic model update** through stochastic gradient descent (SGD):
  $$
  \theta^{t+1} = \theta^t - \eta \nabla_\theta L_{CE}
  $$
  - **Explanation:**
    - $\theta^t$: Current model weights.
    - $\nabla_\theta L_{CE}$: Gradient of the cross-entropy loss as a function of the model weights.
    - $\eta$: Learning rate (controls the step size of each update).

- **LLM training relies on optimizers** that are variants of SGD:
  - Such as Adam (Adaptive Moment Estimation).
  - **Distributed training** setup using many GPUs.


## 3. Types of LLMs

Encoder-only:

- Masked language modelling
- Smaller & older models, but still very good for IE tasks

Decoder-only:

- Now the dominant approach to LLMs

Encoder-decoder:

- Both self-attention and cross-attention
- Employed for tasks such as MT

## 4. Model Alignment

### 4.1. Fine-tuning

Once a model has been (pre)-trained, it can be fine-tuned for a particular task

- Basically “further” training with smaller amounts of in-domain data, taking the pretrained model as starting point
- Much more data-efficient than learning a model from scratch!
    
    → WHY? because the initial model has internalized “semantic representations” that can be transferred to the task to solve
    
- Only OPTIMIZE a small fraction of paramenters → **Parameter-efficient Fine-tuning (PEFT)**
  - *Adapters:* Insert small trainable modules between layers
  - *LoRA (Low-Rank Adaptation):* Modifies the model weights using lowrank matrices
  - *Soft Prompting:* Appends trainable vectors to the input prompt
  - *IA3 (Intrinsic Attention-guided Adaptation):* Adjusts attention mechanism in transformer layers

![PEFT](/static/images/resource/from-transformer-to-llm/PEFT_viz_9.png)


### 4.2. Two Alignment Strategies

- **Instruction Tuning:** `*<instruction, correct response>*`
    - Manually, by asking humans to write a question/problem and a correct answer to that problem
    - By converting existing datasets for supervised NLP tasks (for text classification, sentiment analysis, NER, machine translation, etc.) to prompts → answers
    - By using LLMs to generate instructions-> responses, based on external databases, general guidelines, etc.
- **Preference Alignment:** using **preference data**, such as ratings of answers to prompts
    - Several formats are possible, but a common one is to use triples `*<prompt, chosen response, reject response>*`
    - The ratings can be collected by paying crowdworkers (based on guidelines that define what constitutes a “good” response), or by asking users to rate the responses they get

## 5. Prompting

A **prompt** = **input** to the LLM, which can be a question or instructions on the task to perform

- System prompt: Generic instructions, defined by the system provider, on how the system should respond (be helpful, avoid offensive language, etc.) and how it should present itself to the user (“You are ChatGPT, a large language model trained by OpenAI…”)
- User prompt: Specific instructions provided by the user

![prompting](/static/images/resource/from-transformer-to-llm/prompting_viz_10.png)

### 5.1. In-context learning

For complex tasks, inserting in the prompt a few examples / demonstrations can help the model understand what kind of output is expected
→ Called **“in-context learning”** or **“fewshots prompting”**

### 5.2. Chain-of-Thought

But those “reasoning” capabilities can be improved by forcing the LLM to generate longer, step-by-step responses

- “Let’s think step by step”
- In-context demonstrations of reasoning steps involved in solving similar problems
- Can also ask the model to criticize or debate its own responses

## 6. Generation

Assume we have a trained a generative LLM that takes as input a list of $[w_1, w_2, …, w_n]$ and return a probability distribution over the next token $w_{i+1}$

### 6.1. Greedy Decoding

- At each step, just select the token with highest probability
- And continue until a special `<eos>` token, or the maximum output length

But greedy decoding is **short-sighted** ↔ Can get trapped in local minima (good “local” choice of token, but poor text at the end, with no way to backtrack)

### 6.2 Beam search decoding

- We keep at each step a «beam» of K hypotheses representing the best partial outputs so far
- Those K hypotheses are gradually expanded, token by token
- Until we reach `<eos>` or the maximum output length
- Finally, we take the hypothesis with max probability among the K alternatives

### 6.3. Top-K sampling

- At each step, we select the K tokens with highest probability
- We sample the next token only among those `K` possibilities (after renormalising the distribution)
- And continue until `<eo>` or max sentence length

### 6.4. Top-p sampling

- Same principle, but instead of a fixed K, we select tokens among those making up the top **`p%`** of the probability mass

### 6.5. Temperature

The `creativity` of the generation can be controlled by the temperature
- Temperatures < 1 will make the decoder more “conservative”, increasing its focus on high-probability tokens
- Temperatures > 1 will make the decoder more “creative” and sometimes select less-likely tokens

## 7. Retrieval-augmented generation

Shortcoming of LLMs
- Cannot add, remove or update fatual knowledge without retraining the model
- Lack of transparency difficult to inspect the model knowledge or determine which source in the training data may have influenced the model when answering a given question
- How can we connect LLM to internal databases or to ***specialized knowledge*** sources ?

-> **Retrieval-augmented generation (RAG)**

- Basic idea

  1. Given a user prompt, we first search for relevant documents in a text database
      -> **RETRIEVER** = **Information Retrieval Task**
  2. The most relevant texts are then retrieved and added to the prompt to generate the response
      -> **READER**
    
- Advantages of RAG
  - Easier to **edit** the LLM’s factual knowledge: simply update the underlying database, without needing to retrain the LLM!
  - Cleaner distinction between «linguistic» and «factual» knowledge
  - Better **transparency**: if a model produces a wrong answer, we can check whether the error stems from the retrieved documents
  - Better **attribution**: the LLM can indicate the sources that have been used to generate its response

# Reference

- [Lecture 7 - NN2](https://www.uio.no/studier/emner/matnat/ifi/IN4080/h24/slides/07-nn2.pdf), IN4080, Autumn 2024, University of Oslo
- [Lecture 8 - NN2](https://www.uio.no/studier/emner/matnat/ifi/IN4080/h24/slides/in4080---2024---llms.pdf), IN4080, Autumn 2024, University of Oslo
- Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. Draft of August 20, 2024. CHAPTER 9, The Transformer