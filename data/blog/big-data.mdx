---
title: 'Big Data: Concepts, Architecture, and Technologies'
date: '2025-07-05'
tags: ['Big Data', 'Hadoop', 'Kafka', 'Spark', 'Data Engineering', 'Data Warehouse']
draft: false
summary: 'This article explores the world of Big Data, covering core concepts like the 4Vs (Volume, Velocity, Variety, Veracity), key technologies including Hadoop, Kafka, and Spark, and modern architectures such as Persistent Staging Areas and real-time processing systems. It provides a comprehensive overview of the technologies that emerged to address the challenges of modern data growth beyond traditional database capabilities.'
image: '/static/images/resource/big-data/thumbnail.png'
series: 'Database & Data Engineering'
seriesOrder: 10
---

<TOCInline toc={props.toc} asDisclosure toHeading={4} />

## Introduction

Big Data technologies emerged as traditional OLTP databases became insufficient to handle the explosive growth in data volume and complexity. The early 2000s witnessed a revolution in data generation from various sources—social media, e-commerce, mobile devices, and sensors—creating an unprecedented demand for new processing and storage paradigms.

This article explores the fundamental concepts of Big Data, the technologies developed to handle these massive datasets, and the architectural patterns that have evolved to address modern data challenges.

![Big Data Overview](/static/images/resource/big-data/01bigdata.png)

## Core Concepts of Big Data

Big Data is typically characterized by the "4Vs," which describe the challenges that traditional data processing systems struggle to address:

| Characteristic | Description                                                                                |
| -------------- | ------------------------------------------------------------------------------------------ |
| **Volume**     | The sheer scale of data being generated, often reaching petabytes and beyond               |
| **Velocity**   | The speed at which new data is being created and needs to be processed, often in real-time |
| **Variety**    | The diverse formats of data—structured, semi-structured, and unstructured                  |
| **Veracity**   | The uncertainty, biases, and potential inaccuracies in data requiring verification         |

### Example Use Cases

Big Data technologies are applied across various domains for different types of data processing:

**Transactional Data**

- Fraud detection in financial services
- Real-time stock market analysis
- Telecom event processing

**Sub-Transactional Data**

- Weblog analytics
- Social media sentiment analysis
- Online behavior tracking

## Hadoop and early days landscape

Hadoop emerged as one of the first comprehensive frameworks designed specifically for Big Data processing. Its distributed architecture allows for horizontal scaling by simply adding more commodity hardware nodes.

![Hadoop Ecosystem](/static/images/resource/big-data/02hadoop-architecture.png)

### HDFS (Hadoop Distributed File System)

HDFS is a fault-tolerant distributed file system designed to run on commodity hardware in cloud environments. It provides high-throughput access to large datasets by splitting files into blocks and distributing them across a cluster.

The architecture consists of:

- **NameNode (Master)**: Manages the file system namespace and regulates access to files
- **DataNode (Slave)**: Stores and retrieves blocks as directed by the NameNode

### YARN

YARN, introduced in Hadoop 2, is the resource management platform responsible for managing compute resources on Hadoop clusters. It enables multiple data processing engines to run alongside the traditional MapReduce paradigm.

Key components:

- **Resource Manager**: Allocates resources among all applications in the system
- **Scheduler**: Plans the execution of processes based on resource availability
- **Node Manager**: Monitors resource usage and reports to the Resource Manager

These services work together to optimize execution and resource usage in the Hadoop environment, allowing for more efficient parallel processing.

## Main components of Hadoop

### Cluster (3 nodes)

A typical Hadoop cluster consists of three types of nodes, each serving a specific purpose in the distributed processing framework.

![Hadoop Cluster](/static/images/resource/big-data/03hadoop-hw.png)

#### Edge Node – The Gateway

Edge nodes act as the interface between users and the cluster:

- Users submit jobs and queries through these nodes (e.g., Spark jobs, Hive queries)
- They host client tools and interfaces (e.g., Hue, CLI tools)
- They ensure secure access to the internal cluster

#### Master Nodes – The Brain of the Cluster

Master nodes handle coordination and management of resources and tasks:

- **NameNode (HDFS)**: Tracks where data is stored
- **ResourceManager (YARN)**: Allocates computing resources
- **Hive Metastore / Impala Catalog**: Manages metadata for querying

Multiple master nodes ensure high availability—if one fails, others take over.

#### Worker Nodes – The Muscle

Worker nodes store the actual data and perform computations:

- **DataNodes (HDFS)**: Store file blocks
- **NodeManagers (YARN)**: Execute processing tasks
- **Spark Executors / Hive Workers**: Process data in parallel

The more worker nodes, the more data and jobs a cluster can handle.

### HDFS: Fault-Tolerant Storage

HDFS, which stands for Hadoop Distributed System, is the cloud file system, can be extended as required by simply adding nodes.

The character is to be a **fault-tolerance** file systems, this means that there is no loss of information in case of faults or malfunctions

This is guaranteed by replicated system used by HDFS for storing the files on Cloud. In fact, the file are split into multiple packets and replicated within the HDFS. In this way we have multiple copies of the same file.

### Hive: SQL Interface for Hadoop

Apache Hive provides a data warehouse infrastructure built on top of Hadoop for data summarization, querying, and analysis. It offers a SQL-like interface to Hadoop, making it accessible to users familiar with traditional database systems.

Hive consists of two main services:

- **Hive Server**: The SQL engine that executes queries
- **Metastore**: The table catalog that maintains metadata about tables and partitions

![Hive Architecture](/static/images/resource/big-data/05HDFS.png)

### HBase: Distributed NoSQL Database

HBase is a distributed, scalable, big data store designed to support random, real-time read/write access to large datasets.

Key features:

- Data is partitioned based on RowKeys into Regions
- Each Region contains a range of RowKeys based on their binary order
- A **RegionServer** can contain several **Regions**
- All Regions contained in a **RegionServer** share one write-ahead log (WAL)
- Regions are automatically split if they become too large

## Kafka: Streaming Data Platform

Apache Kafka is a distributed streaming platform that provides three key capabilities:

1. Publishing and subscribing to streams of records
2. Storing streams of records in a fault-tolerant way
3. Processing streams of records as they occur

### Common Use Cases

- **Website Activity Tracking**: Web applications send events such as page views and searches to Kafka, where they become available for real-time processing, dashboards, and offline analytics in Hadoop.
- **Operational Metrics**: Alerting and reporting on operational metrics for system monitoring.
- **Log Aggregation**: Collecting logs from multiple services and making them available in a standard format to multiple consumers.
- **Stream Processing**: Frameworks such as Spark Streaming read data from topics, process it, and write processed data to new topics for consumption by users and applications.

### 4 Core APIs

| API               | Description                                                                                                             |
| ----------------- | ----------------------------------------------------------------------------------------------------------------------- |
| **Producer API**  | Allows applications to publish streams of records to one or more Kafka topics                                           |
| **Consumer API**  | Enables applications to subscribe to topics and process the streams of records                                          |
| **Streams API**   | Supports applications acting as stream processors, transforming input streams to output streams                         |
| **Connector API** | Facilitates building reusable producers or consumers that connect Kafka topics to existing applications or data systems |

<div
  style={{
    padding: '1rem',
    marginBottom: '1rem',
    border: '1px solid var(--tw-prose-quote-borders)',
    borderRadius: '0.5rem',
    backgroundColor: 'var(--tw-prose-quote-bg)',
  }}
>
  <p>
    <strong>Key to Kafka's Performance:</strong> One of the keys to Kafka's high performance is the
    simplicity of the brokers' responsibilities. Topics consist of one or more partitions that are
    ordered, immutable sequences of messages. Since writes to a partition are sequential, this
    design greatly reduces the number of hard disk seeks. Additionally, Kafka brokers are not
    responsible for tracking message consumption—that responsibility falls on the consumers.
  </p>
</div>

## Spark

Spark introduces the concept of Resilient Distributed Datasets (RDDs)—immutable, fault-tolerant collections of objects that can be operated on in parallel.

- RDDs can contain any type of object and are created by loading external datasets or distributing collections from the driver program
- RDDs support two types of operations:
  - **Transformations**: Operations (such as map, filter, join, union) that yield a new RDD containing the result
  - **Actions**: Operations (such as reduce, count, first) that return a value after computation on an RDD

**Spark Core** is the base engine for large-scale parallel and distributed data processing, responsible for:

- Memory management and fault recovery
- Scheduling, distributing, and monitoring jobs on a cluster
- Interacting with storage systems

![Spark Core](/static/images/resource/big-data/06SparkCore.png.png)

Other components include:

- **Spark SQL**: Supports querying data via SQL or Hive Query Language
- **Spark Streaming**: Enables real-time processing of streaming data
- **MLlib**: Provides machine learning algorithms designed to scale out on a cluster
- **GraphX**: Offers a library for manipulating and performing graph-parallel operations

## PSA - Persistent Staging Area

A Persistent Staging Area (PSA) serves as an intermediate storage layer between source systems and analytical data stores.

- Large data storage capacity for both structured and unstructured data
- Ability to trace any data changes over time
- Data always available via both SQL and file interfaces
- Scalability through adding new nodes to the cluster
- Parallel processing engine for analyzing any type of data

![PSA Details](/static/images/resource/big-data/08PSA-details.png)

## Real-time Architecture

Modern Big Data architectures often incorporate real-time processing capabilities to handle data as it arrives, enabling immediate insights and actions.

![Real-time Architecture](/static/images/resource/big-data/09real-time.png)

1. **Real-time Producer**: Devices or sensors that generate raw data
   - Production line aggregators
   - IoT devices, web services, or REST APIs

2. **Broker**: Buffer to prevent data loss during peak data rates
   - Publish/subscribe messaging system for asynchronous exchange
   - Works as a FIFO buffer to ensure all data is eventually processed
   - Publishers are sensors/devices, subscriber is the real-time engine

3. **Real-time Engine**: Processes incoming data for predictions, alarms, or automated actions
   - May be implemented as dedicated software, generic frameworks, or custom solutions

4. **Data Lake**: Storage for raw data used for statistical analysis or data warehouse enrichment
   - Typically stores data as object blobs or files

5. **NoSQL Database**: Used for real-time data reading and writing

6. **Real-time Dashboard**: Displays real-time data visualization

7. **External Applications**: Receive alarms and information extracted from raw data

## Search Engine Integration

Search engines provide powerful capabilities for finding and analyzing data across the Big Data ecosystem.

![Search Engine Architecture](/static/images/resource/big-data/10search-engine.png)

- Creates a "virtual" single repository by fusing structured, unstructured, and federated data
- Provides secure and granular access to all applications and data stores
- Supports tagging, commenting, organizing, and rating content to enhance navigation and discovery
- Offers dynamic clustering and text analysis
- Can ingest domain-specific taxonomies and dictionaries
- Scales horizontally to handle growing data volumes
